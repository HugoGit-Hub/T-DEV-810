{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration (resize images, load images, split dataset into train, validation, test)\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_size = (150, 150)\n",
    "\n",
    "train_dir = '../dataset/train'\n",
    "validation_dir = '../dataset/val'\n",
    "test_dir = '../dataset/test'\n",
    "\n",
    "def resize_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for label in os.listdir(directory):\n",
    "        label_dir = os.path.join(directory, label)\n",
    "        for filename in os.listdir(label_dir):\n",
    "            img_path = os.path.join(label_dir, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, img_size)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "    if (directory == train_dir):\n",
    "        images = images[:3500]\n",
    "        labels = labels[:3500]\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "X_train, y_train = resize_images_from_directory(train_dir)\n",
    "X_validation, y_validation = resize_images_from_directory(validation_dir)\n",
    "X_test, y_test = resize_images_from_directory(test_dir)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pictures\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic samples using SMOTE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def count_files(folder_path):\n",
    "    count = 0\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "folder_path_normal = \"../dataset/train/NORMAL\"\n",
    "folder_path_pneumonia = \"../dataset/train/PNEUMONIA\"\n",
    "num_files_normal = count_files(folder_path_normal)\n",
    "num_files_pneumonia = count_files(folder_path_pneumonia)\n",
    "print(\"Number of files in the folder normal :\", num_files_normal)\n",
    "print(\"Number of files in the folder pneumonia :\", num_files_pneumonia)\n",
    "\n",
    "counter_before = Counter(y_train)\n",
    "print(\"Class Distribution Before SMOTE:\", counter_before)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)  \n",
    "\n",
    "counter_after = Counter(y_train)\n",
    "print(\"Class Distribution After SMOTE:\", counter_after)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(counter_before.keys(), counter_before.values(), color='b', label='Before SMOTE')\n",
    "plt.bar(counter_after.keys(), counter_after.values(), color='r', alpha=0.5, label='After SMOTE')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Instances')\n",
    "plt.title('Class Distribution Before and After SMOTE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the best hyperparameters using GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "param = {\n",
    "    'C':[0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'dual': [True, False],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LinearSVC(), param, cv=5, scoring='accuracy')\n",
    "grid.fit(X_validation, y_validation)\n",
    "\n",
    "best_params = grid.best_params_\n",
    "best_score = grid.best_score_\n",
    "best_estimator = grid.best_estimator_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "print('Score with best estimator:', best_estimator.score(X_test, y_test))\n",
    "\n",
    "confusion_matrix(y_validation, best_estimator.predict(X_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and score it\n",
    "\n",
    "model = LinearSVC(C=0.01, dual=False, loss='squared_hinge', penalty='l1')\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pipeline to scale the data and train the model\n",
    "\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# model = make_pipeline(StandardScaler(), LinearSVC(C=0.01, dual=False, loss='squared_hinge', penalty='l1'))\n",
    "# params = {\n",
    "# }\n",
    "# grid = GridSearchCV(model, params, cv=5, scoring='accuracy')\n",
    "# grid.fit(X_train, y_train)\n",
    "# grid.score(X_test, y_test)\n",
    "\n",
    "# predictions = grid.predict(X_test)\n",
    "\n",
    "# num_correct = sum(y_test == predictions)\n",
    "# total_predictions = len(y_test)\n",
    "\n",
    "# print(num_correct / total_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "indices = np.arange(len(y_test))\n",
    "\n",
    "def success_rate() :\n",
    "    num_correct = sum(y_test == predictions)\n",
    "    total_predictions = len(y_test)\n",
    "    return num_correct / total_predictions\n",
    "\n",
    "def success_rate_per_class(class_name):\n",
    "    expected = y_test[y_test == class_name]\n",
    "    predicted = predictions[y_test == class_name]\n",
    "    num_correct = sum(expected == predicted)\n",
    "    total_predictions = len(expected)\n",
    "    return num_correct / total_predictions\n",
    "\n",
    "print(\"Success Rate:\", success_rate())\n",
    "print(\"Success Rate for Pneumonia:\", success_rate_per_class('PNEUMONIA'))\n",
    "print(\"Success Rate for Normal:\", success_rate_per_class('NORMAL'))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(indices, y_test, label='Expected', marker='o')\n",
    "plt.plot(indices, predictions, label='Predicted', marker='x')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Expected vs. Predicted Results')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
